{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSCI 4253 / 5253 - Lab #4 - Patent Problem with Spark DataFrames\n",
    "<div>\n",
    " <h2> CSCI 4283 / 5253 \n",
    "  <IMG SRC=\"https://www.colorado.edu/cs/profiles/express/themes/cuspirit/logo.png\" WIDTH=50 ALIGN=\"right\"/> </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This [Spark cheatsheet](https://s3.amazonaws.com/assets.datacamp.com/blog_assets/PySpark_SQL_Cheat_Sheet_Python.pdf) is useful as is [this reference on doing joins in Spark dataframe](http://www.learnbymarketing.com/1100/pyspark-joins-by-example/).\n",
    "\n",
    "The [DataBricks company has one of the better reference manuals for PySpark](https://docs.databricks.com/spark/latest/dataframes-datasets/index.html) -- they show you how to perform numerous common data operations such as joins, aggregation operations following `groupBy` and the like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following aggregation functions may be useful -- [these can be used to aggregate results of `groupby` operations](https://docs.databricks.com/spark/latest/dataframes-datasets/introduction-to-dataframes-python.html#example-aggregations-using-agg-and-countdistinct). More documentation is at the [PySpark SQL Functions manual](https://spark.apache.org/docs/2.3.0/api/python/pyspark.sql.html#module-pyspark.sql.functions). Feel free to use other functions from that library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, count, countDistinct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create our session as described in the tutorials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nWhat is happening here? We instantiating a spark session. What does that mean?\\nSpark Session can generate dataframes\\n\\nWhat is builder?\\n\\nMaster is telling YARN to use all available local cores, right?\\n\\ngetOrCreate checks if [*] is existing and retrieves it or creates it appropriately?\\n\\n~~~~\\n\\nSparkConf: Probably the one that generates the RDD\\n\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Lab4-Dataframe\") \\\n",
    "    .master(\"local[*]\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the citations and patents data and check that the data makes sense. Note that unlike in the RDD solution, the data is automatically inferred to be Integer() types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the citations gzip file\n",
    "# Citations is now a pyspark.sql.dataframe.DataFrame\n",
    "citations = spark.read.load('cite75_99.txt.gz',\n",
    "            format=\"csv\", sep=\",\", header=True,\n",
    "            compression=\"gzip\",\n",
    "            inferSchema=\"true\")\n",
    "\n",
    "# Read in the patent gzip file\n",
    "# Patents is now a pyspark.sql.dataframe.DataFrame\n",
    "patents = spark.read.load('apat63_99.txt.gz',\n",
    "            format=\"csv\", sep=\",\", header=True,\n",
    "            compression=\"gzip\",\n",
    "            inferSchema=\"true\")\n",
    "\n",
    "citations.write.parquet(\"citations.parquet\")\n",
    "patents.write.parquet(\"patents.parquet\")\n",
    "\n",
    "# Length of patRdd = 2_923_922\n",
    "# Length of citRdd = 16_522_438"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Citations:\n",
      "+-------+-------+\n",
      "| CITING|  CITED|\n",
      "+-------+-------+\n",
      "|3858241| 956203|\n",
      "|3858241|1324234|\n",
      "|3858241|3398406|\n",
      "|3858241|3557384|\n",
      "|3858241|3634889|\n",
      "+-------+-------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "Patents:\n",
      "+-------+-----+-----+-------+-------+-------+--------+-------+------+------+---+------+-----+--------+--------+-------+--------+--------+--------+--------+--------+--------+--------+\n",
      "| PATENT|GYEAR|GDATE|APPYEAR|COUNTRY|POSTATE|ASSIGNEE|ASSCODE|CLAIMS|NCLASS|CAT|SUBCAT|CMADE|CRECEIVE|RATIOCIT|GENERAL|ORIGINAL|FWDAPLAG|BCKGTLAG|SELFCTUB|SELFCTLB|SECDUPBD|SECDLWBD|\n",
      "+-------+-----+-----+-------+-------+-------+--------+-------+------+------+---+------+-----+--------+--------+-------+--------+--------+--------+--------+--------+--------+--------+\n",
      "|3070801| 1963| 1096|   null|     BE|   null|    null|      1|  null|   269|  6|    69| null|       1|    null|    0.0|    null|    null|    null|    null|    null|    null|    null|\n",
      "|3070802| 1963| 1096|   null|     US|     TX|    null|      1|  null|     2|  6|    63| null|       0|    null|   null|    null|    null|    null|    null|    null|    null|    null|\n",
      "|3070803| 1963| 1096|   null|     US|     IL|    null|      1|  null|     2|  6|    63| null|       9|    null| 0.3704|    null|    null|    null|    null|    null|    null|    null|\n",
      "|3070804| 1963| 1096|   null|     US|     OH|    null|      1|  null|     2|  6|    63| null|       3|    null| 0.6667|    null|    null|    null|    null|    null|    null|    null|\n",
      "|3070805| 1963| 1096|   null|     US|     CA|    null|      1|  null|     2|  6|    63| null|       1|    null|    0.0|    null|    null|    null|    null|    null|    null|    null|\n",
      "+-------+-----+-----+-------+-------+-------+--------+-------+------+------+---+------+-----+--------+--------+-------+--------+--------+--------+--------+--------+--------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, countDistinct\n",
    "\n",
    "# Start a Spark Session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Lab4-Dataframe\") \\\n",
    "    .master(\"local[*]\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read in the data from parquet\n",
    "citations = spark.read.parquet(\"citations.parquet\")\n",
    "patents = spark.read.parquet(\"patents.parquet\")\n",
    "\n",
    "# Confirm data was read in correctly\n",
    "print(\"Citations:\")\n",
    "citations.show(5)\n",
    "print()\n",
    "print(\"Patents:\")\n",
    "patents.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the Patent-State Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "States:\n",
      "+-------+-------+\n",
      "| PATENT|POSTATE|\n",
      "+-------+-------+\n",
      "|3070802|     TX|\n",
      "|3070803|     IL|\n",
      "|3070804|     OH|\n",
      "|3070805|     CA|\n",
      "|3070806|     PA|\n",
      "+-------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the patent - state information & drop all null values\n",
    "states = patents.select(['PATENT','POSTATE']).na.drop()\n",
    "print(\"States:\")\n",
    "states.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join the Patent-State Table with the Cited-Citing Table (x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "| CITING|count|\n",
      "+-------+-----+\n",
      "|6009554|    8|\n",
      "|6009551|    2|\n",
      "|6009550|    4|\n",
      "|6009549|    4|\n",
      "|6009543|    1|\n",
      "|6009541|   43|\n",
      "|6009540|    4|\n",
      "|6009539|    3|\n",
      "|6009538|    1|\n",
      "|6009536|    1|\n",
      "+-------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "citationResults = citations.join(states, citations.CITING == states.PATENT, 'left') \\\n",
    "                  .drop('PATENT').withColumnRenamed('POSTATE', 'CITING STATE') \\\n",
    "                  .join(states, citations.CITED == states.PATENT, 'left') \\\n",
    "                  .drop('PATENT').withColumnRenamed('POSTATE', 'CITED STATE')\n",
    "\n",
    "# Reduce to columns that are costate citations\n",
    "citationResults = citationResults[citationResults[\"CITING STATE\"] == citationResults[\"CITED STATE\"]] \\\n",
    "                  .groupBy('CITING').count().sort(\"CITING\", ascending=False)\n",
    "\n",
    "\n",
    "citationResults.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Append to the original Patent Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+-----+-------+-------+-------+--------+-------+------+------+---+------+-----+--------+--------+-------+--------+--------+--------+--------+--------+--------+--------+-------------------+\n",
      "| PATENT|GYEAR|GDATE|APPYEAR|COUNTRY|POSTATE|ASSIGNEE|ASSCODE|CLAIMS|NCLASS|CAT|SUBCAT|CMADE|CRECEIVE|RATIOCIT|GENERAL|ORIGINAL|FWDAPLAG|BCKGTLAG|SELFCTUB|SELFCTLB|SECDUPBD|SECDLWBD|# COSTATE CITATIONS|\n",
      "+-------+-----+-----+-------+-------+-------+--------+-------+------+------+---+------+-----+--------+--------+-------+--------+--------+--------+--------+--------+--------+--------+-------------------+\n",
      "|5959466| 1999|14515|   1997|     US|     CA|    5310|      2|  null|   326|  4|    46|  159|       0|     1.0|   null|  0.6186|    null|  4.8868|  0.0455|   0.044|    null|    null|                125|\n",
      "|5983822| 1999|14564|   1998|     US|     TX|  569900|      2|  null|   114|  5|    55|  200|       0|   0.995|   null|  0.7201|    null|   12.45|     0.0|     0.0|    null|    null|                103|\n",
      "|6008204| 1999|14606|   1998|     US|     CA|  749584|      2|  null|   514|  3|    31|  121|       0|     1.0|   null|  0.7415|    null|     5.0|  0.0085|  0.0083|    null|    null|                100|\n",
      "|5952345| 1999|14501|   1997|     US|     CA|  749584|      2|  null|   514|  3|    31|  118|       0|     1.0|   null|  0.7442|    null|  5.1102|     0.0|     0.0|    null|    null|                 98|\n",
      "|5998655| 1999|14585|   1998|     US|     CA|    null|      1|  null|   560|  1|    14|  114|       0|     1.0|   null|  0.7387|    null|  5.1667|    null|    null|    null|    null|                 96|\n",
      "|5958954| 1999|14515|   1997|     US|     CA|  749584|      2|  null|   514|  3|    31|  116|       0|     1.0|   null|  0.7397|    null|   5.181|     0.0|     0.0|    null|    null|                 96|\n",
      "|5936426| 1999|14466|   1997|     US|     CA|    5310|      2|  null|   326|  4|    46|  178|       0|     1.0|   null|    0.58|    null| 11.2303|  0.0765|   0.073|    null|    null|                 94|\n",
      "|5739256| 1998|13983|   1995|     US|     CA|   70060|      2|    15|   528|  1|    15|  453|       0|     1.0|   null|  0.8232|    null| 15.1104|  0.1124|  0.1082|    null|    null|                 90|\n",
      "|5925042| 1999|14445|   1997|     US|     CA|  733846|      2|  null|   606|  3|    32|  242|       0|     1.0|   null|  0.7382|    null|  8.3471|     0.0|     0.0|    null|    null|                 90|\n",
      "|5951547| 1999|14501|   1997|     US|     CA|  733846|      2|  null|   606|  3|    32|  242|       0|     1.0|   null|  0.7382|    null|  8.3471|     0.0|     0.0|    null|    null|                 90|\n",
      "+-------+-----+-----+-------+-------+-------+--------+-------+------+------+---+------+-----+--------+--------+-------+--------+--------+--------+--------+--------+--------+--------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Append to the original list\n",
    "patTemp = patents.join(citationResults, patents['PATENT'] == citationResults['CITING'], 'left') \\\n",
    "          .drop('CITING').withColumnRenamed('count', '# COSTATE CITATIONS').fillna({'# COSTATE CITATIONS': '0'})\n",
    "          \n",
    "patTemp.sort('# COSTATE CITATIONS', ascending=False).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Steps for Lab\n",
    "1. [Citing #] [Cited #] => [Citing #] [Citing State] [Cited #]\n",
    "2. [Citing #] [Citing State] [Cited #] => [Citing #] [Citing State] [Cited #] [Cited State]\n",
    "3. Filter to only include lines with identical [Citing State] and [Cited State]\n",
    "4. Count number lines per [Citing #] => Append to [Citing #] Info.\n",
    "5. Return full list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actions for RDD objects\n",
    "\n",
    "Some of the most simple actions are:\n",
    "* count() - Return the number of items in the RDD\n",
    "* take(n) - Extract and return the first n items from the RDD. Returns a python List[pyspark.sql.types.Row]\n",
    "* first() - Same as take(1)\n",
    "* collect() - Same as take(count()) - **returns full RDD**\n",
    "* takeSample(_withReplacement_:Boolean, _num_:int, [ seed:Int] ) - extract a random set of _num_ items from the RDD with or without replacement.\n",
    "* takeOrdered( _num_ ) - extract _num_ items from the sorted RDD.\n",
    "\n",
    "\n",
    "# Actions for DataFrame Objects\n",
    "* select(['ColumnName']) - returns a new DF object with just the requested columns\n",
    "* show(n=20) - Show the first n items of the dataframe\n",
    "* count() - Return the number of items in the DataFrame\n",
    "* columns - Return a list of column names\n",
    "* dtypes - Return a list of tuples of form [(Column_Name, Data_Type)]\n",
    "* toDF(['new', 'column', 'names']) - Returns a new DF object. Input: list of new column names\n",
    "* withColumnRenamed('old', 'new') - Returns a new DF object. Inputs (2): Old column name to be replaced, New column name. Can only replace one column name at a time\n",
    "* drop('colName') - Returns a new DF object with the specified column dropped. Unclear if you cna put in multiple column names as a list\n",
    "* df[df.colName < value] - Returns a new DF object that only has rows with the corresponding criteria - SINGLE DROP\n",
    "* df[(df.colName1 < value1) & (df.colName1 < value1)] - Same as above - MULTIPLE DROP\n",
    "* df.withColumn('gpm', 1 / df.mpg) - Returns a new DF object with first agr as the col name and 2nd arg as the operation. Divide by 0 returns null\n",
    "* df.fillna(val) returns a new df with"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Notes\n",
    "\n",
    "Terminology:\n",
    "\n",
    "Slices = Partitions\n",
    "\n",
    "MAIN SPARK ABSTRACTIONS: <br>\n",
    "RDD: Distributed collection of objects <br>\n",
    "DataFrame: Distributed dataset of tabular data (integrated SQL, ML Algorithms)\n",
    "\n",
    "Important Concepts: <br>\n",
    "Immutability <br>\n",
    "Lazy computing\n",
    "\n",
    "/////////\n",
    "\n",
    "RDD operations:\n",
    "\n",
    "Length of rDD = rdd.count()\n",
    "\n",
    "/////////\n",
    "\n",
    "conf = <class 'pyspark.conf.SparkConf'>\n",
    "sc = <class 'pyspark.context.SparkContext'>\n",
    "sc.parallelize is a thing\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
